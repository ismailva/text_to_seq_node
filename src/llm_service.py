from typing import List, get_args
from functools import lru_cache
from src.text_to_seq_prompt import TextToSeqPrompt
import logging
from src.llm_model import LLMModel
from src import constants

logger = logging.getLogger()


class LLMService:
    def __init__(self):
        self.model = LLMModel.get_model(constants.DEFAULT_LLM)
        self.text_to_seq_prompt = TextToSeqPrompt()

    @lru_cache(maxsize=64)
    def text_to_node(self, user_text:str) -> str:
        arguments = self.text_to_seq_prompt.convert_to_model_required_format(self.model, user_text)
        user_output = self.model.invoke(**arguments)
        logger.info("node sequence is generated by model.")
        return user_output
    
if __name__ == "__main__":
    ls=LLMService()
    res=ls.text_to_node("Reduce a list of scores to find the highest score and log the result.")
    print(res)
